{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxG5JWKZSqV7",
        "outputId": "b033b4d8-7ed2-4f47-cea7-2ada131eba1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/transformers/text.zip\" /content/"
      ],
      "metadata": {
        "id": "0kbeQpdxV8T8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip text.zip -d ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKC3nVRIWGQB",
        "outputId": "180fba7f-070c-43a1-9c29-d456d798c28b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  text.zip\n",
            "  inflating: ./wikisent2.txt         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 1500000 wikisent2.txt > wiki_small.txt\n"
      ],
      "metadata": {
        "id": "BQfYAVrNWwEr"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "3Y0YtgOEXjmp"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = 'wiki_small.txt'\n",
        "model_prefix = 'wiki_spm'\n",
        "\n",
        "vocab_size = 12000\n",
        "\n",
        "block_size = 128\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "lr = 3e-4\n",
        "\n",
        "embed_dim = 320\n",
        "num_heads = 5\n",
        "num_layers = 6\n",
        "ff_dim = 1280\n",
        "dropout = 0.1\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CizVeZr0Y_Ce",
        "outputId": "4c8878d2-1fb3-4f9b-b144-619138dab95c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f'{model_prefix}.model'):\n",
        "\n",
        "  print('Training tokenizer....')\n",
        "\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input = corpus,\n",
        "      model_prefix = model_prefix,\n",
        "      vocab_size = vocab_size,\n",
        "      model_type = 'bpe',\n",
        "      character_coverage = 1.0,\n",
        "\n",
        "      pad_id = 0, pad_piece = '<pad>',\n",
        "      unk_id = 1, unk_piece = '<unk>',\n",
        "      bos_id = 2, bos_piece = '<s>',\n",
        "      eos_id = 3, eos_piece = '</s>',\n",
        "\n",
        "      hard_vocab_limit = False\n",
        "  )\n",
        "\n",
        "  print('Tokenizer trained!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJVW4c_uZnSa",
        "outputId": "325524f6-0d64-427a-f2d8-638e8c74f22e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokenizer....\n",
            "Tokenizer trained!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('wiki_spm.model')\n",
        "\n",
        "vocab_size = sp.get_piece_size()\n",
        "\n",
        "print('Vocab : ', vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg1GsGtFbIyY",
        "outputId": "8cd8537c-429e-43c7-bffc-fcaf83c1565c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab :  12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokens(path):\n",
        "\n",
        "  tokens = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "\n",
        "      if not line:\n",
        "        continue\n",
        "\n",
        "      ids = sp.encode(line)\n",
        "\n",
        "      tokens.extend(ids)\n",
        "\n",
        "  return torch.tensor(tokens, dtype = torch.long)"
      ],
      "metadata": {
        "id": "1QGrgDrPbnqu"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading data.....')\n",
        "\n",
        "tokens = load_tokens(corpus)\n",
        "\n",
        "print('Total tokens : ', len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqmesS_BcEYC",
        "outputId": "126f687b-c241-46b4-cd75-4a046391dc96"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data.....\n",
            "Total tokens :  43705641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gptdataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.block_size\n",
        "        end = start + self.block_size\n",
        "\n",
        "        x = self.data[start:end]\n",
        "        y = self.data[start+1:end+1]\n",
        "\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "3wmqmAL6cyWl"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Gptdataset(tokens, block_size)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    pin_memory = True\n",
        ")"
      ],
      "metadata": {
        "id": "tip3H9VfdfYQ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Multihead(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, heads, dropout):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    assert dim % heads == 0\n",
        "\n",
        "    self.dim = dim\n",
        "    self.heads = heads\n",
        "    self.head_dim = dim // heads\n",
        "\n",
        "    self.wq = nn.Linear(dim, dim)\n",
        "    self.wk = nn.Linear(dim, dim)\n",
        "    self.wv = nn.Linear(dim, dim)\n",
        "\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    B, T, D = x.shape\n",
        "\n",
        "    Q = self.wq(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
        "    K = self.wk(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
        "    V = self.wv(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "    scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "    attn = F.softmax(scores, dim = -1)\n",
        "\n",
        "    attn = self.dropout(attn)\n",
        "\n",
        "    out = attn @ V\n",
        "\n",
        "    out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "\n",
        "    return self.proj(out)"
      ],
      "metadata": {
        "id": "PsUByq3Sdyll"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, heads, ff_dim, dropout):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(dim)\n",
        "\n",
        "    self.attn = Multihead(dim, heads, dropout)\n",
        "\n",
        "    self.ln2 = nn.LayerNorm(dim)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(dim, ff_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(ff_dim, dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    x = x + self.attn(self.ln1(x), mask)\n",
        "\n",
        "    x = x + self.ff(self.ln2(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "lPewgkp3hP5d"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyGpt(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
        "\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    self.blocks = nn.ModuleList([\n",
        "        DecoderBlock(\n",
        "            embed_dim, num_heads, ff_dim, dropout\n",
        "        )\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    self.ln_f = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, vocab_size, bias = False)\n",
        "\n",
        "    self.head.weight = self.token_emb.weight\n",
        "\n",
        "    mask = torch.triu(\n",
        "        torch.ones(block_size, block_size),\n",
        "        diagonal = 1\n",
        "    ).bool()\n",
        "\n",
        "    self.register_buffer('mask', mask)\n",
        "\n",
        "  def forward(self, idx):\n",
        "\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok = self.token_emb(idx)\n",
        "\n",
        "    pos = self.pos_emb(torch.arange(T, device = idx.device))\n",
        "\n",
        "    x = self.drop(tok + pos)\n",
        "\n",
        "    mask = self.mask[:T, :T]\n",
        "\n",
        "    for block in self.blocks:\n",
        "\n",
        "      x = block(x, mask)\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "\n",
        "    logits = self.head(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, max_new, temp = 1.0, top_k = None):\n",
        "\n",
        "    for _ in range(max_new):\n",
        "\n",
        "      idx_cond = idx[:, -block_size :]\n",
        "      logits = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :] / temp\n",
        "\n",
        "      if top_k:\n",
        "\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "\n",
        "        logits[logits < v[:, [-1]]] = -float('inf')\n",
        "\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      next_id = torch.multinomial(probs, 1)\n",
        "\n",
        "      idx = torch.cat([idx, next_id], dim = 1)\n",
        "\n",
        "    return idx\n",
        ""
      ],
      "metadata": {
        "id": "UeFK7H83h-ku"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "Iqyhi5WLlMHF"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# TRAINING\n",
        "# =====================\n",
        "\n",
        "model = TinyGpt().to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "print(\"Params:\",\n",
        "      sum(p.numel() for p in model.parameters()) / 1e6,\n",
        "      \"M\")\n",
        "\n",
        "\n",
        "print(\"Start training...\")\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "\n",
        "        logits = model(x)\n",
        "\n",
        "\n",
        "        loss = criterion(\n",
        "            logits.view(-1, vocab_size),\n",
        "            y.view(-1)\n",
        "        )\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "        if step % 200 == 0:\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch+1} | Step {step} | Loss {loss.item():.4f}\"\n",
        "            )\n",
        "    training_loss = total_loss / len(loader)\n",
        "    train_ppl = math.exp(training_loss)\n",
        "\n",
        "    print(\n",
        "    f\"Epoch {epoch+1} | \"\n",
        "    f\"Train Loss: {training_loss:.4f} | \"\n",
        "    f\"Train Perplexity: {train_ppl:.2f}\"\n",
        ")\n",
        "\n",
        "\n",
        "    torch.save(\n",
        "        model.state_dict(),\n",
        "        f\"custom_gpt_epoch{epoch+1}.pt\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLjOa3ACkcF8",
        "outputId": "3f109e8b-80d7-42d6-9ee4-94a7a48569db"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: 11.27936 M\n",
            "Start training...\n",
            "Epoch 1 | Step 0 | Loss 199.8322\n",
            "Epoch 1 | Step 200 | Loss 13.7480\n",
            "Epoch 1 | Step 400 | Loss 9.0216\n",
            "Epoch 1 | Step 600 | Loss 7.8364\n",
            "Epoch 1 | Step 800 | Loss 7.4863\n",
            "Epoch 1 | Step 1000 | Loss 7.2884\n",
            "Epoch 1 | Step 1200 | Loss 6.9972\n",
            "Epoch 1 | Step 1400 | Loss 6.9242\n",
            "Epoch 1 | Step 1600 | Loss 6.9822\n",
            "Epoch 1 | Step 1800 | Loss 6.9059\n",
            "Epoch 1 | Step 2000 | Loss 6.9021\n",
            "Epoch 1 | Step 2200 | Loss 6.5821\n",
            "Epoch 1 | Step 2400 | Loss 6.5872\n",
            "Epoch 1 | Step 2600 | Loss 6.4783\n",
            "Epoch 1 | Step 2800 | Loss 6.6564\n",
            "Epoch 1 | Step 3000 | Loss 6.5224\n",
            "Epoch 1 | Step 3200 | Loss 6.6152\n",
            "Epoch 1 | Step 3400 | Loss 6.3901\n",
            "Epoch 1 | Step 3600 | Loss 6.1267\n",
            "Epoch 1 | Step 3800 | Loss 6.2728\n",
            "Epoch 1 | Step 4000 | Loss 6.1299\n",
            "Epoch 1 | Step 4200 | Loss 6.0691\n",
            "Epoch 1 | Step 4400 | Loss 5.8115\n",
            "Epoch 1 | Step 4600 | Loss 6.2690\n",
            "Epoch 1 | Step 4800 | Loss 5.9307\n",
            "Epoch 1 | Step 5000 | Loss 5.9419\n",
            "Epoch 1 | Step 5200 | Loss 5.6430\n",
            "Epoch 1 | Step 5400 | Loss 6.0375\n",
            "Epoch 1 | Step 5600 | Loss 5.7412\n",
            "Epoch 1 | Step 5800 | Loss 5.9053\n",
            "Epoch 1 | Step 6000 | Loss 5.6604\n",
            "Epoch 1 | Step 6200 | Loss 5.7771\n",
            "Epoch 1 | Step 6400 | Loss 5.9812\n",
            "Epoch 1 | Step 6600 | Loss 5.7763\n",
            "Epoch 1 | Step 6800 | Loss 5.2349\n",
            "Epoch 1 | Step 7000 | Loss 5.6123\n",
            "Epoch 1 | Step 7200 | Loss 5.7733\n",
            "Epoch 1 | Step 7400 | Loss 5.1629\n",
            "Epoch 1 | Step 7600 | Loss 5.3611\n",
            "Epoch 1 | Step 7800 | Loss 5.5229\n",
            "Epoch 1 | Step 8000 | Loss 5.4532\n",
            "Epoch 1 | Step 8200 | Loss 5.6065\n",
            "Epoch 1 | Step 8400 | Loss 5.2938\n",
            "Epoch 1 | Step 8600 | Loss 5.2591\n",
            "Epoch 1 | Step 8800 | Loss 5.5564\n",
            "Epoch 1 | Step 9000 | Loss 5.2090\n",
            "Epoch 1 | Step 9200 | Loss 5.5835\n",
            "Epoch 1 | Step 9400 | Loss 5.3368\n",
            "Epoch 1 | Step 9600 | Loss 5.3118\n",
            "Epoch 1 | Step 9800 | Loss 5.2893\n",
            "Epoch 1 | Step 10000 | Loss 5.4121\n",
            "Epoch 1 | Step 10200 | Loss 5.1524\n",
            "Epoch 1 | Step 10400 | Loss 5.5673\n",
            "Epoch 1 | Step 10600 | Loss 5.4217\n",
            "Epoch 1 | Train Loss: 6.5322 | Train Perplexity: 686.91\n",
            "Epoch 2 | Step 0 | Loss 5.2355\n",
            "Epoch 2 | Step 200 | Loss 5.4138\n",
            "Epoch 2 | Step 400 | Loss 5.1054\n",
            "Epoch 2 | Step 600 | Loss 5.2013\n",
            "Epoch 2 | Step 800 | Loss 5.2834\n",
            "Epoch 2 | Step 1000 | Loss 5.1493\n",
            "Epoch 2 | Step 1200 | Loss 5.4756\n",
            "Epoch 2 | Step 1400 | Loss 5.4254\n",
            "Epoch 2 | Step 1600 | Loss 4.8846\n",
            "Epoch 2 | Step 1800 | Loss 4.8913\n",
            "Epoch 2 | Step 2000 | Loss 5.1195\n",
            "Epoch 2 | Step 2200 | Loss 4.7871\n",
            "Epoch 2 | Step 2400 | Loss 4.9818\n",
            "Epoch 2 | Step 2600 | Loss 4.9280\n",
            "Epoch 2 | Step 2800 | Loss 4.8636\n",
            "Epoch 2 | Step 3000 | Loss 4.9344\n",
            "Epoch 2 | Step 3200 | Loss 5.1446\n",
            "Epoch 2 | Step 3400 | Loss 5.1488\n",
            "Epoch 2 | Step 3600 | Loss 4.8790\n",
            "Epoch 2 | Step 3800 | Loss 4.9149\n",
            "Epoch 2 | Step 4000 | Loss 4.9426\n",
            "Epoch 2 | Step 4200 | Loss 4.8559\n",
            "Epoch 2 | Step 4400 | Loss 4.9193\n",
            "Epoch 2 | Step 4600 | Loss 4.8523\n",
            "Epoch 2 | Step 4800 | Loss 5.0040\n",
            "Epoch 2 | Step 5000 | Loss 4.9941\n",
            "Epoch 2 | Step 5200 | Loss 4.9650\n",
            "Epoch 2 | Step 5400 | Loss 5.0821\n",
            "Epoch 2 | Step 5600 | Loss 4.8896\n",
            "Epoch 2 | Step 5800 | Loss 5.0157\n",
            "Epoch 2 | Step 6000 | Loss 4.5602\n",
            "Epoch 2 | Step 6200 | Loss 4.9839\n",
            "Epoch 2 | Step 6400 | Loss 4.9006\n",
            "Epoch 2 | Step 6600 | Loss 4.9691\n",
            "Epoch 2 | Step 6800 | Loss 4.8041\n",
            "Epoch 2 | Step 7000 | Loss 4.9564\n",
            "Epoch 2 | Step 7200 | Loss 4.8062\n",
            "Epoch 2 | Step 7400 | Loss 5.0158\n",
            "Epoch 2 | Step 7600 | Loss 5.0293\n",
            "Epoch 2 | Step 7800 | Loss 4.8479\n",
            "Epoch 2 | Step 8000 | Loss 4.7220\n",
            "Epoch 2 | Step 8200 | Loss 4.9022\n",
            "Epoch 2 | Step 8400 | Loss 4.7310\n",
            "Epoch 2 | Step 8600 | Loss 4.9873\n",
            "Epoch 2 | Step 8800 | Loss 4.7597\n",
            "Epoch 2 | Step 9000 | Loss 4.7920\n",
            "Epoch 2 | Step 9200 | Loss 5.0489\n",
            "Epoch 2 | Step 9400 | Loss 4.6486\n",
            "Epoch 2 | Step 9600 | Loss 4.7229\n",
            "Epoch 2 | Step 9800 | Loss 4.6444\n",
            "Epoch 2 | Step 10000 | Loss 4.7700\n",
            "Epoch 2 | Step 10200 | Loss 4.7814\n",
            "Epoch 2 | Step 10400 | Loss 4.8686\n",
            "Epoch 2 | Step 10600 | Loss 4.6689\n",
            "Epoch 2 | Train Loss: 4.9105 | Train Perplexity: 135.71\n",
            "Epoch 3 | Step 0 | Loss 4.7618\n",
            "Epoch 3 | Step 200 | Loss 4.7105\n",
            "Epoch 3 | Step 400 | Loss 4.8660\n",
            "Epoch 3 | Step 600 | Loss 4.5503\n",
            "Epoch 3 | Step 800 | Loss 4.3150\n",
            "Epoch 3 | Step 1000 | Loss 4.3717\n",
            "Epoch 3 | Step 1200 | Loss 4.7449\n",
            "Epoch 3 | Step 1400 | Loss 4.7133\n",
            "Epoch 3 | Step 1600 | Loss 4.7564\n",
            "Epoch 3 | Step 1800 | Loss 4.7416\n",
            "Epoch 3 | Step 2000 | Loss 4.4179\n",
            "Epoch 3 | Step 2200 | Loss 4.3374\n",
            "Epoch 3 | Step 2400 | Loss 4.5505\n",
            "Epoch 3 | Step 2600 | Loss 4.7160\n",
            "Epoch 3 | Step 2800 | Loss 4.7053\n",
            "Epoch 3 | Step 3000 | Loss 4.6794\n",
            "Epoch 3 | Step 3200 | Loss 4.5869\n",
            "Epoch 3 | Step 3400 | Loss 4.3306\n",
            "Epoch 3 | Step 3600 | Loss 4.5619\n",
            "Epoch 3 | Step 3800 | Loss 4.7339\n",
            "Epoch 3 | Step 4000 | Loss 4.6105\n",
            "Epoch 3 | Step 4200 | Loss 4.3412\n",
            "Epoch 3 | Step 4400 | Loss 4.6723\n",
            "Epoch 3 | Step 4600 | Loss 4.4343\n",
            "Epoch 3 | Step 4800 | Loss 4.6915\n",
            "Epoch 3 | Step 5000 | Loss 4.6420\n",
            "Epoch 3 | Step 5200 | Loss 4.7366\n",
            "Epoch 3 | Step 5400 | Loss 4.6014\n",
            "Epoch 3 | Step 5600 | Loss 4.5199\n",
            "Epoch 3 | Step 5800 | Loss 4.3340\n",
            "Epoch 3 | Step 6000 | Loss 4.2292\n",
            "Epoch 3 | Step 6200 | Loss 4.4239\n",
            "Epoch 3 | Step 6400 | Loss 4.3658\n",
            "Epoch 3 | Step 6600 | Loss 4.6242\n",
            "Epoch 3 | Step 6800 | Loss 4.4272\n",
            "Epoch 3 | Step 7000 | Loss 4.4530\n",
            "Epoch 3 | Step 7200 | Loss 4.4764\n",
            "Epoch 3 | Step 7400 | Loss 4.3356\n",
            "Epoch 3 | Step 7600 | Loss 4.4968\n",
            "Epoch 3 | Step 7800 | Loss 4.4912\n",
            "Epoch 3 | Step 8000 | Loss 4.7664\n",
            "Epoch 3 | Step 8200 | Loss 4.4895\n",
            "Epoch 3 | Step 8400 | Loss 4.4730\n",
            "Epoch 3 | Step 8600 | Loss 4.5390\n",
            "Epoch 3 | Step 8800 | Loss 4.2247\n",
            "Epoch 3 | Step 9000 | Loss 4.2331\n",
            "Epoch 3 | Step 9200 | Loss 4.4715\n",
            "Epoch 3 | Step 9400 | Loss 4.3519\n",
            "Epoch 3 | Step 9600 | Loss 4.3709\n",
            "Epoch 3 | Step 9800 | Loss 4.3854\n",
            "Epoch 3 | Step 10000 | Loss 4.4425\n",
            "Epoch 3 | Step 10200 | Loss 4.4362\n",
            "Epoch 3 | Step 10400 | Loss 4.5203\n",
            "Epoch 3 | Step 10600 | Loss 4.4836\n",
            "Epoch 3 | Train Loss: 4.5366 | Train Perplexity: 93.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- SAMPLE ---\\n\")\n",
        "\n",
        "\n",
        "prompt = \"Forest\"\n",
        "\n",
        "ids = sp.encode(prompt)\n",
        "\n",
        "x = torch.tensor(ids).unsqueeze(0).to(device)\n",
        "\n",
        "out = model.generate(x, 100, temp=0.8, top_k=40)\n",
        "\n",
        "print(sp.decode(out[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mCLjR_FJ7g8",
        "outputId": "06d79e5a-37b4-4053-ad5e-d96231f31e22"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SAMPLE ---\n",
            "\n",
            "Forest, he was given his best-school stroke with his best-selling album in a \"Hiptes\" and \"PLise's last song as it was recorded. A member of his best-known team, he was the youngest final round with a half-brother competition. A member of his live live album, he became a member of the band in 1953. A member of his success on his debut single and he played one season with his best-selling\n"
          ]
        }
      ]
    }
  ]
}